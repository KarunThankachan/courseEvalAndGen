

The previous page focused on the metrics for evaluating supervised learning
problems. The presence of labeled data makes it somewhat straightforward to
train and test the model's performance. Now, we will focus on metrics that can
be used when labeled data is not present. There are two approaches to
evaluating clustering. The Internal and External evaluation approaches. The
internal approach involves summarizing the clustering task to a single quality
score, while the external approach compares the clustering to a ground truth
classification; ground truth is empirical evidence or data that is provable.
Clustering can also be evaluated by an expert.

  * Internal Evaluation evaluates the clusters with high similarity within the cluster and high dissimilarity with other clusters and assigns the clusters a score. The cluster with the best score is seen to be the best. Internal evaluation although useful, can have its drawbacks. It gives insight into how one clustering technique performs against another but it can not speak to the validity of the results in the clusters. A sound example from wikipedia that illustrates this: k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means, nor of an evaluation criterion that assumes convexity, is sound.

Let's look at internal evaluation techniques that are used to assess the
quality of clustering methods:

Silhouette Coefficient shows how similar a data point is to its cluster
compared to other clusters. It is calculated using the mean intra cluster
distance and the mean nearest cluster distance for each data point. A
silhouette coefficient is 1 is best and -1 is seen to be the worst and means
that the sample is in the wrong cluster, when the silhouette coefficient is
close to 0, there is a presence of overlapping clusters.

Dunn Index is also used to evaluate clustering techniques and similar to the
Silhouette coefficient, it is dependent on the data within the clusters. A
good clustering is one with a higher Dunn index. When using this evaluation
technique, you want to be aware of a high computational cost when you have a
large number of clusters. The Dunn index is computed by calculating the
distance between each data point in a cluster and others in different
clusters. The minimum of the pairwise distance is used to determine minimum
separation (min.separation). The compactness of a cluster is measured by
computing the distance between the data in the same cluster (Max.diameter).
Finally, the Dunn index will be:

min.separation/max.diameter

If the data set contains compact and well-separated clusters, the diameter of
the clusters is expected to be small and the distance between the clusters is
expected to be large. Thus, Dunn index should be maximized.

  * External Evaluation measures the results from a clustering task based on data not used for the clustering task. Benchmarks are set from a set of pre-classified data. External evaluation techniques need ground truth data to evaluate clustering. 

Rand Index tells you how similar a cluster or clusters are to a set benchmark.
This is similar a classification evaluation technique. You can calculate the
Rand index thus:

(TP + TN)/(TP+FP+FN+TN)

Purity is considered a no frills technique that assigns each cluster to a
class (usually one that occurs often in the cluster), the number of correctly
assigned observations is divided by the overall number of observations to
determine accuracy. Purity close to 1 is best and close to 0 is not optimal. A
large number of clusters can lead to a higher purity. There is a tradeoff
between quality of clustering and number of clusters when using purity as a
metric. The normalized mutual information (NMI) can be used to measure and
compare the quality of clustering between different clusterings with varying
number of clusters.

Jaccard Index is used in cluster analysis evaluation and convolutional neural
networks among others. It is defined as "the size of the intersection divided
by the size of the union of the sample sets." The Jaccard distance measures
dissimilarity between sample sets.

F-Measure is simply computed as the 2 * ((Precision * Recall)/(Precision +
Recall)). You might remember it from the classification metrics, it is also
known as the F1 score.

Dice Index also known as the Sorensen-Dice indexor Dice Coefficient can assess
the similarity of two samples. It ranges from 0 to 1. Dice index is a
semimetric version of Jaccard index and gives less weight to outliers in a
dataset. It is used to measure the lexical association score of two words.

Reading: Clustering Evaluation Techniques (20min Read)  
  
---  
  * You should also be interested in measuring to what degree clusters exist in the data to be clustered before beginning the cluster analysis. This is sometimes done by comparing a dataset against another (one without clusters). The Hopkins statistic is used to measure cluster tendency. A resulting value of 1 or close enough will show that the data is clustered. Data that is uniformly distributed will be closer to 0. Hopkins statistic is quite good at estimating randomness in a dataset. 

