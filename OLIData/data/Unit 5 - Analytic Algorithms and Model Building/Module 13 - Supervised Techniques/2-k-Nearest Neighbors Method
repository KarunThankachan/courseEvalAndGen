

Have you come across the term, lazy-learning"? This is a method in which
training data is generalized and most useful for large datasets that will be
updated continuously. In this case, a model typically depends on (or queries)
a small number of attributes in the dataset. An application of a lazy learner
is a feature of a recommendation system. A recommendation system will
continuously update new movies, or new items to shoppers preferences and the
recommender relies on certain variables such as ratings, pricing, and country
of origin. The opposite of a lazy learning method is an eager learning method.
An eager learner requires less space than a lazy learner (remember lazy
learners are continuously updating its dataset), an eager learner will learn
immediately and sometimes for a long time but will take a shorter time to
classify data. A lazy learner will learn for a short time but take a longer
time to classify data.

A well known lazy learner is the k-Nearest Neighbor! This method can be used
to solve both classification and regression problems. kNN is considered rather
simple yet useful (it is also simple to implement in Python or R) and is one
of the first algorithms or methods that entrants to data science will learn.

kNN will "find a predefined number of training samples closest in distance to
the new point or a new observation, and predict the label for the new
observation". kNN can also suffer from the curse of dimensionality. This
method will perform best when data is rescaled. It is best practice to
normalize applicable data to the range of 0,1 and to standardize the data if
it has a gaussian distribution.

kNN will perform its task just in time as it does not learn the model,
remember the characteristic of a lazy learner? It spends less time learning
and more time classifying or predicting.

There are three steps involved in making predictions with k-NN: Calculate the
euclidean distance, identify nearest neighbor(s) and then perform task.

Reading: k-Nearest Neighbors: From Global to Local  
  
---  
  
k-Nearest Neighbors method begins by identifying the observations (or k
records) in the training dataset that are similar to a new observation that
should be classified. The neighboring observations can be used to classify the
new observation into its class. That class should be its predominant class.
Since this method is considered a non-parametric method, it will take
information from similarities between the predictor values of the observations
in the dataset. The kNN method uses distance computations to determine
distance between each new observation and the observations in the training
dataset. The most popular distance measure used in kNN is the Euclidean
Distance. The formula for the Euclidean distance between observations (xi and
yi) is shown below:

Euclidean Distance Function. Source- Sayad-2010

There are other distance measures including Manhattan (distance between
vectors using the sum of their absolute differences) and Minkowski distances.
Euclidean is used because it is computationally cheap, keep in mind that
predictors should be standardized before implementing the Euclidean distance
function. Euclidean distance will not work with categorical variables unless
they are converted into binary dummy variables. An alternative distance
measure in this case would be the Hamming Distance, which can be used as long
as you do not have more than two (2) classes.

Once distances are computed, a rule is set to determine the class that a new
observation will be assigned and this is based on the classes of its
neighbors. Now, you start to see why kNN is considered a similarity function.
How do you determine the number of neighbors to assess, so that a new
observation or data point can be classified correctly? You can determine that
the new data should be classified in the same class as its single
closest/nearest neighbor i.e. k = 1. Can you guess what will happen in this
situation? You are right if you guessed that you will face overfitting. There
are options to reduce this issue and that is by using a k that is greater than
1. If you assign k = n i.e. n being number of observations in the dataset,
there will be over smoothing and all new data will be assigned to the majority
class. Values of k are historically between 3 and 10 and usually an odd number
to avoid ties1. Sometimes k  can be chosen using cross-validation, and the
validation dataset will validate the best k.

kNN had been described for the purposes of predicting a categorical response
but it is also effective for predicting continuous value responses just like
you would with a linear regression model.

  * The main difference between predicting a categorical response and a continuous value is that the algorithm will use the average (usually weighted) response of the neighbors of the new data point to determine the accurate prediction. The weight will decrease as the distance increases from the data point at which the prediction is required. 

  * The best k for a classification task is assessed using the overall error rate but in this instance, the best k is determined using the RMS error. 

