{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer, BertModel, DistilBertModel\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import pathlib\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "MAX_LEN = 50\n",
    "\n",
    "def tokenizerfnc(str):\n",
    "    return tokenizer.encode(str,max_length=MAX_LEN, pad_to_max_length=True, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader - for validation and test\n",
    "class ConceptLoader(Dataset):\n",
    "    def __init__(self, file_name):\n",
    "        self.data = []\n",
    "        with open(file_name, newline='') as file:\n",
    "            data_reader = csv.reader(file, delimiter=',')\n",
    "        # tokenization\n",
    "        for row in data_reader:\n",
    "            tokenized_concept_0 = tokenizerfnc(row[0])\n",
    "            attn_masks_0 = [int(word!=0) for word in tokenized_concept_0]\n",
    "            tokenized_concept_1 = tokenizerfnc(row[1])\n",
    "            attn_masks_1 = [int(word!=0) for word in tokenized_concept_0]\n",
    "            feats = row[2:]\n",
    "            self.data.append( [torch.LongTensor(tokenized_concept_0), torch.LongTensor(attn_masks_0),\n",
    "                               torch.LongTensor(tokenized_concept_1), torch.LongTensor(attn_masks_1),\n",
    "                           torch.LongTensor(feats) ] )\n",
    "                           \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "###########################\n",
    "# LOSS\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#########################\n",
    "# HYPER PARAMETER TUNING\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConceptEmbedding,self).__init__()\n",
    "        self.embed_size = 768 # TODO: Find the BERT Embedding size\n",
    "        self.hidden_size = 256\n",
    "        self.nlayers = 2\n",
    "        self.out_size = 2\n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.lstm1 = nn.LSTM(input_size = self.embed_size, hidden_size = self.hidden_size, num_layers= self.nlayers)\n",
    "        self.lstm2 = nn.LSTM(input_size = self.embed_size, hidden_size = self.hidden_size, num_layers= self.nlayers) \n",
    "\n",
    "        # TODO, include other features\n",
    "        # self.mlp =  ADD IN MLP\n",
    "\n",
    "    def forward(self, x1, x2, attn_mask1, attn_mask2, others):\n",
    "        x1, _ = self.bert_layer(x1, attention_mask = attn_mask1)\n",
    "        x1 = x1[:, 1:].permute(1,0,2) # BxLxH -> LxBxH\n",
    "        h1, _ = self.lstm1(x1)\n",
    "        h1 = h1[-1]\n",
    "        \n",
    "        x2, _ = self.bert_layer(x2, attention_mask = attn_mask2)\n",
    "        x2 = x2[:, 1:].permute(1,0,2) # BxLxH -> LxBxH\n",
    "        h2, _ = self.lstm2(x2)\n",
    "        h2 = h2[-1]\n",
    "        # TODO\n",
    "        # Bring in reference features\n",
    "        return h1,h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# HELPER FUNCTIONS\n",
    "\n",
    "def train(model, train_loader, epoch):\n",
    "    '''\n",
    "    Args:\n",
    "\n",
    "    Ret: \n",
    "    '''\n",
    "    total = 0\n",
    "    accuracy = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []    \n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    for batch_num, (feats1,label1,attn_mask1, feats2,label2,attn_mask2) in enumerate(train_loader):\n",
    "        feats = feats.to(DEVICE)\n",
    "        label = label1.to(DEVICE)\n",
    "        attn_mask = attn_mask.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(feats, attn_mask)\n",
    "\n",
    "        # task loss\n",
    "        loss = criterion1(out, label1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if batch_num % 50 == 0:\n",
    "            print(\"Batch \", batch_num, \" done. Loss is \", loss.item())\n",
    "\n",
    "        # getting predictions\n",
    "        _, pred_labels = torch.max(F.softmax(out_task2, dim=1), 1)\n",
    "        all_labels.extend(list(np.array(label2.cpu())))\n",
    "        predictions = pred_labels.cpu()\n",
    "        all_predictions.extend(list(np.array(predictions)))\n",
    "\n",
    "        del(feats)\n",
    "        del(label)\n",
    "        del(attn_mask)\n",
    "\n",
    "    running_loss /= len(train_loader)\n",
    "    print(\"Loss of epoch \", (epoch+1), \" is \", running_loss)\n",
    "    score = f1_score(all_labels, all_predictions)\n",
    "    print(\"Train Score: \", score)\n",
    "    #print(all_predictions)\n",
    "    return\n",
    "\n",
    "\n",
    "def validate(model, valid_loader, epoch):\n",
    "    '''\n",
    "    Args:\n",
    "\n",
    "    Ret:\n",
    "    '''\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    for batch_num, (feats,label,attn_mask) in enumerate(valid_loader):\n",
    "        feats = feats1.to(DEVICE)\n",
    "        label = label1.to(DEVICE)\n",
    "        attn_mask = attn_mask1.to(DEVICE)\n",
    "        out = model.evaluate(feats, attn_mask)\n",
    "\n",
    "        _, pred_labels = torch.max(F.softmax(out_task1, dim=1), 1)\n",
    "        all_labels.extend(list(np.array(label1.cpu())))\n",
    "        predictions = pred_labels.cpu()\n",
    "        all_predictions.extend(list(np.array(predictions)))\n",
    "\n",
    "        del(feats)\n",
    "        del(label)\n",
    "        del(attn_mask)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # calculuate f1-scores\n",
    "    score = f1_score(all_labels, all_predictions)\n",
    "    print(\"Test score: \", score)\n",
    "\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
