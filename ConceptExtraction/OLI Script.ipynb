{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import html2text\n",
    "import shutil\n",
    "\n",
    "browser = webdriver.Chrome(executable_path='C:\\\\Users\\\\bhara\\\\Downloads\\\\chromedriver.exe') \n",
    "url = \"svn.oli.cmu.edu/svn/content/editor/projects/echo-oli-cmu-edu/foundations-zxfk9cu6/branches/v_1_0-echo/content/x-oli-workbook_page/\"\n",
    "#url = \"svn.oli.cmu.edu/svn/content/editor/projects/echo-oli-cmu-edu/foundations-zxfk9cu6/branches/v_1_3-echo/content/x-oli-workbook_page/\"\n",
    "text = browser.get(\"https://bgaind:rGCpX8Jad6_q@\" + url)\n",
    "text = browser.page_source\n",
    "\n",
    "soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "content = []\n",
    "titles = []\n",
    "cur_urls = []\n",
    "for i in soup.find(\"ul\").findAll(\"li\"):\n",
    "    cur_url = i.find(\"a\")[\"href\"]\n",
    "    cur_urls.append(cur_url)\n",
    "    text = browser.get(\"https://bgaind:rGCpX8Jad6_q@\" + url + \"/\" + cur_url)\n",
    "    text = browser.page_source\n",
    "    content.append(text)\n",
    "    soup1 = BeautifulSoup(text, 'html.parser')\n",
    "    titles.append(soup1.find(\"title\").string.strip().replace(\":\", \"-\").replace(\"?\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"data1/\"\n",
    "manual_titles = []\n",
    "f = open(\"modules\", \"r\")\n",
    "modules_lines = f.readlines()\n",
    "f.close()\n",
    "shutil.rmtree(DIR)\n",
    "os.mkdir(DIR)\n",
    "cur_unit = \"\"\n",
    "cur_module = \"\"\n",
    "cur_chapter_idx = 0\n",
    "asd = \"\"\n",
    "h = html2text.HTML2Text()\n",
    "h.ignore_emphasis = True\n",
    "all_content_parsed = []\n",
    "for i in modules_lines:\n",
    "    line = i.strip()\n",
    "    if line.find(\"Unit\") != -1:\n",
    "        os.mkdir(DIR + line)\n",
    "        cur_unit = line\n",
    "    elif line.find(\"Module\") != -1:\n",
    "        os.mkdir(DIR + cur_unit + \"/\" + line)\n",
    "        cur_module = line\n",
    "        cur_chapter_idx = 0\n",
    "    else:\n",
    "        if line.find(\"Overview,\") !=-1 or line.find(\"Summary,\") !=-1:\n",
    "            id1 = line.split(\",\")[1]\n",
    "            name = line.split(\",\")[0]\n",
    "            for c in content:\n",
    "                soup1 = BeautifulSoup(c, 'html.parser')\n",
    "                if soup1.find(\"ul\")!= None and soup1.find(\"ul\").has_attr(\"id\") and soup1.find(\"ul\")[\"id\"] == id1:\n",
    "                    f = open(DIR + \"/\" + cur_unit + \"/\" + cur_module + \"/\" + str(cur_chapter_idx+1) + \"-\" + name, \"wb\")\n",
    "                    c_write = h.handle(c)\n",
    "                    cidx = c_write.find(\"This XML file does not appear to have any\")\n",
    "                    c_write = c_write[:cidx].encode(\"utf-8\")\n",
    "                    f.write(c_write)\n",
    "                    f.close()\n",
    "                    all_content_parsed.append(c_write)\n",
    "                    cur_chapter_idx += 1\n",
    "                    break\n",
    "                for p in soup1.findAll(\"p\"):\n",
    "                    if p[\"id\"] == id1:\n",
    "                        f = open(DIR + \"/\" + cur_unit + \"/\" + cur_module + \"/\" + str(cur_chapter_idx+1) + \"-\" + name, \"wb\")\n",
    "                        c_write = h.handle(c)\n",
    "                        cidx = c_write.find(\"This XML file does not appear to have any\")\n",
    "                        c_write = c_write[:cidx].encode(\"utf-8\")\n",
    "                        f.write(c_write)\n",
    "                        f.close()\n",
    "                        all_content_parsed.append(c_write)\n",
    "                        cur_chapter_idx += 1\n",
    "                        break\n",
    "        elif line not in titles or titles.count(line) > 1:\n",
    "            print(line)\n",
    "        else:\n",
    "            idx = titles.index(line)\n",
    "            c = content[idx]\n",
    "            f = open(DIR + \"/\" + cur_unit + \"/\" + cur_module + \"/\" + str(cur_chapter_idx+1) + \"-\" + line, \"wb\")\n",
    "            c_write = h.handle(c)\n",
    "            cidx = c_write.find(\"This XML file does not appear to have any\")\n",
    "            c_write = c_write[:cidx].encode(\"utf-8\")\n",
    "            all_content_parsed.append(c_write)\n",
    "            f.write(c_write)\n",
    "            f.close()\n",
    "            cur_chapter_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import PyPDF2\n",
    "import sys\n",
    "import fitz\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def parseIndexText(text):\n",
    "    '''\n",
    "    Given Index of the format\n",
    "    `Cloud Computing ................. 74, 81`\n",
    "    It will extract as [\"Concept Page Numbers\", ..]\n",
    "    '''\n",
    "    contents = text.splitlines()\n",
    "    content_with_pns = []\n",
    "    for i in range(len(contents)):\n",
    "        # check if the next line is numbers only\n",
    "        if (len(content_with_pns) > 1) and (re.search('[0-9]+',content_with_pns[-1]) is None):\n",
    "            content_with_pns[-1] =  content_with_pns[-1] + \" \" + contents[i]\n",
    "        elif re.search('[a-zA-Z]+', contents[i]) is not None:   \n",
    "            # if not add to list\n",
    "            content_with_pns.append(contents[i])\n",
    "        elif len(content_with_pns) > 1:\n",
    "            # if yes append to last element for format `Cloud Computing ...... 74, 81 \\n 32,22`\n",
    "            temp = content_with_pns[-1]  \n",
    "            temp = temp + contents[i]\n",
    "            content_with_pns[-1] = temp\n",
    "    return content_with_pns\n",
    "\n",
    "    \n",
    "def extractConceptsFromIndexPage(content_with_pns):\n",
    "    '''\n",
    "    Extract concepts given the index page of a text book\n",
    "    '''\n",
    "    concept_dict = {}\n",
    "\n",
    "    for content in content_with_pns:\n",
    "        concepts = content.split(',')\n",
    "        curr_concepts = []\n",
    "        curr_pages = []\n",
    "        for concept in concepts:\n",
    "            # name of the concepts\n",
    "            if re.search('[a-zA-Z]+', concept) is not None:\n",
    "                curr_concepts.append(concept.strip())\n",
    "            else:\n",
    "            # page number of concept\n",
    "                concept = concept.strip()\n",
    "                if len(concept) > 0:\n",
    "                    curr_pages.append(concept.replace(\"â€“\",\"-\").strip())\n",
    "\n",
    "        if len(curr_pages) > 0:\n",
    "            concept_dict.update({curr_concept:curr_pages for curr_concept in curr_concepts})\n",
    "\n",
    "    return concept_dict\n",
    "\n",
    "\n",
    "# Entry Point\n",
    "def extractConceptFromIndex(path='', filename=\"Cloud Computing Bible.pdf\", indexPages=(497,527), output=\"\"):\n",
    "    '''\n",
    "    To load data from pdf\n",
    "    Args : filename, name of pdf file\n",
    "    Rets : the list of concepts from the PDF\n",
    "    '''\n",
    "    # Ref : https://www.geeksforgeeks.org/working-with-pdf-files-in-python/\n",
    "    doc = fitz.open(filename)\n",
    "    all_concepts_dict = {}\n",
    "\n",
    "    for pageNumber in range(*indexPages):\n",
    "        page = doc.loadPage(pageNumber)\n",
    "        text = page.getText()\n",
    "        content_with_pns = parseIndexText(text)\n",
    "        all_concepts_dict.update(extractConceptsFromIndexPage(content_with_pns))\n",
    "    \n",
    "    json_dict = json.dumps(all_concepts_dict)\n",
    "    f = open(output + filename+\"_concepts.json\", \"w+\")\n",
    "    f.write(json_dict)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    # return {concepts:[page numbers, ....]}\n",
    "    concept_names_raw = list(all_concepts_dict.keys())\n",
    "    concept_names = []\n",
    "    for concept_name in concept_names_raw:\n",
    "        if (concept_name not in concept_names) and (concept_name+\"s\" \n",
    "                    not in concept_names) and (concept_name[:-1] not in concept_names):\n",
    "            concept_names.append(concept_name)\n",
    "\n",
    "    return concept_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mupdf: expected object number\n"
     ]
    }
   ],
   "source": [
    "concept_names = extractConceptFromIndex(filename=\"Foundations of Data Science - Cornell CS.pdf\", indexPages=(465, 469))\n",
    "concept_names += extractConceptFromIndex(filename=\"Cloud Computing Bible.pdf\", indexPages=(497,527))\n",
    "concept_names += extractConceptFromIndex(filename=\"pythondatasciencehandbook.pdf\", indexPages=(534, 547))\n",
    "\n",
    "concepts_found = set()\n",
    "for c in all_content_parsed:\n",
    "    ccur = c.decode()\n",
    "    for concept in concept_names:\n",
    "        if ccur.find(concept) != -1:\n",
    "            concepts_found.add(concept)\n",
    "f = open(\"concepts\", \"w\")\n",
    "for i in concepts_found:\n",
    "    f.write(i + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bayesian',\n",
       " 'Bell',\n",
       " 'Bias',\n",
       " 'Boolean',\n",
       " 'Calc',\n",
       " 'Central Limit Theorem',\n",
       " 'Clustering',\n",
       " 'Compute',\n",
       " 'Eigenvalue',\n",
       " 'Eigenvector',\n",
       " 'Engage',\n",
       " 'Excel',\n",
       " 'Facebook',\n",
       " 'Google',\n",
       " 'Internet',\n",
       " 'Learning',\n",
       " 'Matplotlib',\n",
       " 'Median',\n",
       " 'Microsoft',\n",
       " 'Netflix',\n",
       " 'Networks',\n",
       " 'Note',\n",
       " 'PCA',\n",
       " 'Pandas',\n",
       " 'Python',\n",
       " 'State',\n",
       " 'Storage',\n",
       " 'Trace',\n",
       " 'Twitter',\n",
       " 'US',\n",
       " 'Variance',\n",
       " 'Word',\n",
       " 'absolute value',\n",
       " 'access',\n",
       " 'analysis',\n",
       " 'application development',\n",
       " 'applications',\n",
       " 'attributes',\n",
       " 'authentication',\n",
       " 'authorization',\n",
       " 'billing',\n",
       " 'boosting',\n",
       " 'categories of',\n",
       " 'challenges',\n",
       " 'characteristics',\n",
       " 'classification',\n",
       " 'classification task',\n",
       " 'clients',\n",
       " 'clustering',\n",
       " 'collaboration',\n",
       " 'communications',\n",
       " 'compliance',\n",
       " 'components',\n",
       " 'computing',\n",
       " 'confusion matrix',\n",
       " 'connectivity',\n",
       " 'controlling',\n",
       " 'creating',\n",
       " 'cross-validation',\n",
       " 'data analysis',\n",
       " 'data as',\n",
       " 'data integrity',\n",
       " 'data management',\n",
       " 'data point',\n",
       " 'data science',\n",
       " 'data security',\n",
       " 'data services',\n",
       " 'datasets',\n",
       " 'debugging',\n",
       " 'decision trees',\n",
       " 'defined',\n",
       " 'defining',\n",
       " 'detecting',\n",
       " 'development',\n",
       " 'dimensionality',\n",
       " 'dimensionality reduction',\n",
       " 'discovery',\n",
       " 'discrete',\n",
       " 'discussions',\n",
       " 'documentation',\n",
       " 'dropping',\n",
       " 'ease of use',\n",
       " 'end users',\n",
       " 'errors',\n",
       " 'extension',\n",
       " 'feature',\n",
       " 'feature engineering',\n",
       " 'feature engineering and',\n",
       " 'filling',\n",
       " 'fitting',\n",
       " 'for dimensionality reduction',\n",
       " 'formula',\n",
       " 'friends',\n",
       " 'governance',\n",
       " 'groups',\n",
       " 'handling',\n",
       " 'hyperparameters',\n",
       " 'image',\n",
       " 'images',\n",
       " 'incremental',\n",
       " 'indexing',\n",
       " 'infrastructure',\n",
       " 'instances',\n",
       " 'integers',\n",
       " 'integration',\n",
       " 'interaction',\n",
       " 'interfaces',\n",
       " 'interoperability',\n",
       " 'intervals',\n",
       " 'k-means',\n",
       " 'k-means clustering',\n",
       " 'labeling',\n",
       " 'left',\n",
       " 'levels',\n",
       " 'lists',\n",
       " 'machine learning',\n",
       " 'management tools',\n",
       " 'maps',\n",
       " 'maximizing',\n",
       " 'media files',\n",
       " 'model validation',\n",
       " 'modules',\n",
       " 'monitoring',\n",
       " 'multinomial',\n",
       " 'multiple',\n",
       " 'network',\n",
       " 'neural networks',\n",
       " 'number of',\n",
       " 'objects',\n",
       " 'obstacles',\n",
       " 'of errors',\n",
       " 'one-hot encoding',\n",
       " 'optimization',\n",
       " 'outliers',\n",
       " 'output',\n",
       " 'overfitting',\n",
       " 'partitioning',\n",
       " 'pattern',\n",
       " 'performance',\n",
       " 'personalized',\n",
       " 'perspective',\n",
       " 'policies',\n",
       " 'population data',\n",
       " 'porting',\n",
       " 'presence',\n",
       " 'products',\n",
       " 'project management',\n",
       " 'project management tool',\n",
       " 'protocols',\n",
       " 'regression',\n",
       " 'regression task',\n",
       " 'responsibilities',\n",
       " 'right',\n",
       " 'roles',\n",
       " 'rules',\n",
       " 'scaling',\n",
       " 'second',\n",
       " 'security',\n",
       " 'services',\n",
       " 'silos',\n",
       " 'similar',\n",
       " 'simple',\n",
       " 'simple linear regression',\n",
       " 'software',\n",
       " 'sorting',\n",
       " 'splitting',\n",
       " 'standards',\n",
       " 'state',\n",
       " 'storage',\n",
       " 'structured',\n",
       " 'subsets',\n",
       " 'supervised',\n",
       " 'supervised learning',\n",
       " 'systems',\n",
       " 'table',\n",
       " 'tail',\n",
       " 'targeted advertising',\n",
       " 'television',\n",
       " 'testing',\n",
       " 'text',\n",
       " 'theorem',\n",
       " 'time series',\n",
       " 'two-dimensional',\n",
       " 'underfitting',\n",
       " 'utility',\n",
       " 'value',\n",
       " 'variance',\n",
       " 'virtualization',\n",
       " 'visualization of',\n",
       " 'web',\n",
       " 'working with'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concepts_found"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
